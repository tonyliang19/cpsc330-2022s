import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from sklearn.compose import ColumnTransformer, make_column_transformer
from sklearn.dummy import DummyClassifier
from sklearn.impute import SimpleImputer
from sklearn.model_selection import cross_val_score, cross_validate, train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.pipeline import Pipeline, make_pipeline
from sklearn.preprocessing import OneHotEncoder, StandardScaler, OrdinalEncoder
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier


census_df = pd.read_csv("adult.csv")
census_df.shape


census_df.head()


train_df, test_df = train_test_split(census_df, test_size=0.50, random_state=10)
X_train, y_train = (
    train_df.drop(columns=["income"]),
    train_df["income"],
)

X_test, y_test = (
    test_df.drop(columns=["income"]),
    test_df["income"],
)



train_df.sort_index()


train_df_nan = train_df.replace("?", np.nan)
test_df_nan = test_df.replace("?", np.nan)
train_df_nan.shape


train_df_nan.describe(include="all")

num_train = train_df_nan[["age","fnlwgt", "education.num", "capital.gain", "capital.loss","hours.per.week"]]
num_train.hist(bins=50, figsize=(25,20));
0


# Fill in the lists below.
# It's OK to keep some of the lists empty or add new lists.

# Check education.num (might be categorical instead)
numeric_features = ["age", "fnlwgt", "education.num", "capital.gain", "capital.loss", 
                    "hours.per.week"]
categorical_features = ["occupation", "workclass", "marital.status",
                        "relationship", "race", "sex",
                        "native.country"]
ordinal_features = ["education"]
binary_features = ["sex"]
drop_features = []
passthrough_features = ["sex"]
target = "income"


X_train, y_train = train_df_nan.drop(columns=["income"]), train_df_nan[["income"]]
X_test, y_test = test_df_nan.drop(columns=["income"]), test_df_nan[["income"]]


X_train["education"].unique()



education_levels = ["Preschool", "1st-4th", "5th-6th",
                    "7th-8th", "9th", "10th", "11th",
                    "12th", "HS-grad", "Some-college", "Assoc-voc",
                    "Assoc-acdm", "Bachelors", "Masters",
                    "Doctorate", "Prof-school"]


preprocessor = make_column_transformer(
    (
        make_pipeline(SimpleImputer(), StandardScaler()),
        numeric_features,
    ),  # scaling on numeric features
    (OneHotEncoder(sparse=False), categorical_features),  # OHE on categorical features
    (
        OrdinalEncoder(categories=[education_levels], dtype=int),
        ordinal_features,
    ),  # Ordinal encoding on ordinal features
    ("passthrough", passthrough_features),  # no transformations on the binary features
    ("drop", drop_features),  # drop the drop features
)


transformed = preprocessor.fit_transform(X_train)


results_dict = {}  # dictionary to store all the results


def mean_std_cross_val_scores(model, X_train, y_train, **kwargs):
    """
    Returns mean and std of cross validation

    Parameters
    ----------
    model :
        scikit-learn model
    X_train : numpy array or pandas DataFrame
        X in the training data
    y_train :
        y in the training data

    Returns
    ----------
        pandas Series with mean scores from cross_validation
    """

    scores = cross_validate(model, X_train, y_train, **kwargs)

    mean_scores = pd.DataFrame(scores).mean()
    std_scores = pd.DataFrame(scores).std()
    out_col = []

    for i in range(len(mean_scores)):
        out_col.append((f"%0.3f (+/- %0.3f)" % (mean_scores[i], std_scores[i])))

    return pd.Series(data=out_col, index=mean_scores.index)


pipe = make_pipeline(transformed, DummyClassifier(strategy="prior"))
#model = pipe.fit(X_train,y_train)
#mean_std_cross_val_scores(model, X_train, y_train, **kwargs)


models = {
    "decision tree": DecisionTreeClassifier(),
    "kNN": KNeighborsClassifier(),
    "RBF SVM": SVC(),
}


param_grid = {"C": np.logspace(-2, 2, 5)}
param_grid



