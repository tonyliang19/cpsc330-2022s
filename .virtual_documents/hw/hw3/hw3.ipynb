import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from sklearn.compose import ColumnTransformer, make_column_transformer
from sklearn.dummy import DummyClassifier
from sklearn.impute import SimpleImputer
from sklearn.model_selection import cross_val_score, cross_validate, train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.pipeline import Pipeline, make_pipeline
from sklearn.preprocessing import OneHotEncoder, StandardScaler, OrdinalEncoder
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier


census_df = pd.read_csv("adult.csv")
census_df.head()


train_df, test_df = train_test_split(census_df, test_size=0.50, random_state=10)
X_train, y_train = (
    train_df.drop(columns=["income"]),
    train_df["income"],
)

X_test, y_test = (
    test_df.drop(columns=["income"]),
    test_df["income"],
)


train_df.sort_index()


train_df_nan = train_df.replace("?", np.nan)
test_df_nan = test_df.replace("?", np.nan)
train_df_nan.head()


train_df_nan.describe(include="all")


# Add ons to plot by levels of "income"
num_train = train_df_nan[["age", "fnlwgt", "education.num", "capital.gain",
                          "capital.loss", "hours.per.week", "income"]]

num_train.hist(bins=50, figsize=(25, 20));


# Fill in the lists below.
# It's OK to keep some of the lists empty or add new lists.

# Check education.num (might be categorical instead)
numeric_features = ["age", "fnlwgt", "education.num", "capital.gain",
                    "capital.loss", "hours.per.week"]

categorical_features = ["workclass", "occupation", "marital.status",
                        "relationship", "native.country"]

ordinal_features = ["education"]
binary_features = ["sex"]
drop_features = ["race"]
passthrough_features = []  # "fnlwgt", "education.num"
target = "income"


train_df_nan.head()


col = "sex"
# train_df_nan[[col]]
train_df_nan[col].unique()


X_train, y_train = train_df_nan.drop(columns=["income"]), train_df_nan[["income"]]
X_test, y_test = test_df_nan.drop(columns=["income"]), test_df_nan[["income"]]
# svm = SVC(kernel="rbf", C=10, gamma=0.1).fit(X_train, y_train)
# The SVC model should not work, as we havent correctly transform features yet


print(X_train["education"].unique())
education_levels = ["Preschool", "1st-4th", "5th-6th",
                    "7th-8th", "9th", "10th", "11th",
                    "12th", "HS-grad", "Some-college", "Assoc-voc",
                    "Assoc-acdm", "Bachelors", "Masters",
                    "Doctorate", "Prof-school"]
print(education_levels)


preprocessor = make_column_transformer(
    (
        make_pipeline(SimpleImputer(), StandardScaler()),
        numeric_features,
    ),  # scaling on numeric features
    (OneHotEncoder(sparse=False), categorical_features),  # OHE on categorical features
    (
        OrdinalEncoder(categories=[education_levels], dtype=int),
        ordinal_features,
    ),  # Ordinal encoding on ordinal features
    (
        OneHotEncoder(drop="if_binary", dtype=int),
        binary_features,
    ),
    ("passthrough", passthrough_features),  # no transformations on the binary features
    ("drop", drop_features),  # drop the drop features
)


transformed = preprocessor.fit_transform(X_train)
transformed.shape


results_dict = {}  # dictionary to store all the results


def mean_std_cross_val_scores(model, X_train, y_train, **kwargs):
    """
    Returns mean and std of cross validation

    Parameters
    ----------
    model :
        scikit-learn model
    X_train : numpy array or pandas DataFrame
        X in the training data
    y_train :
        y in the training data

    Returns
    ----------
        pandas Series with mean scores from cross_validation
    """

    scores = cross_validate(model, X_train, y_train, **kwargs)

    mean_scores = pd.DataFrame(scores).mean()
    std_scores = pd.DataFrame(scores).std()
    out_col = []

    for i in range(len(mean_scores)):
        out_col.append((f"%0.3f (+/- %0.3f)" % (mean_scores[i], std_scores[i])))

    return pd.Series(data=out_col, index=mean_scores.index)


dummy_pipe = make_pipeline(preprocessor, DummyClassifier(strategy="prior"))
results_dict["prior"] = mean_std_cross_val_scores(
    dummy_pipe, X_train, y_train, return_train_score=True, cv=5
)
pd.DataFrame(results_dict).T


models = {
    "decision tree": DecisionTreeClassifier(),
    "kNN": KNeighborsClassifier(),
    "RBF SVM": SVC(),
}
decision_pipe = make_pipeline(preprocessor, models["decision tree"])
knn_pipe = make_pipeline(preprocessor, models["kNN"])
svm_pipe = make_pipeline(preprocessor, models["RBF SVM"])


results_dict["decision"] = mean_std_cross_val_scores(
        decision_pipe, X_train, y_train, return_train_score=True, cv=5
    )
results_dict["knn"] = mean_std_cross_val_scores(
        knn_pipe, X_train, y_train, return_train_score=True, cv=5
    )
results_dict["svm"] = mean_std_cross_val_scores(
        svm_pipe, X_train, y_train, return_train_score=True, cv=5
    )

results = pd.DataFrame(results_dict).T


param_grid = {"C": np.logspace(-2, 2, 5)}
param_grid



